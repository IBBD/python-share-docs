干爬虫也将近一个月了，自己开发了好几个爬虫，从一个新手到可以完成爬虫的常见任务，再到追求开发一个易用性好，性能高，耦合性低，具有良好可扩展性的爬虫目标上奋斗着，这期间积累了一些爬虫开发的简单的人生经验，总结与分享一下：
- **0，入门教程。** scrapy入门教程网上一大堆，这里不做重复的事，需要的请到网上搜索或者直接看scrapy的文档。
- **1，大局观。** 爬虫主要分为两种，一种是像搜索引擎的那种大规模全网式的抓取，一种是针对某些网站，某些主题进行的小规模的抓取，前者抓取的信息比较单一，后者抓取的信息比较多样。我接下来所说的是后者。在爬虫开发中，有两个东西是最主要的：一是需要抓取的URL，二是每个页面的需要抓取的数据，爬虫开发的许多技术难点以及影响爬虫性能的地方都是跟这两个关键点相关。从一个开发者角度来说，如果你有一个网站需要爬取，那么你首先要考虑，你需要爬取的数据在哪些页面，很多时候那些数据并不在一级页面，所以你需要通过其他页面才能进入该页面去获取数据。在这个时候，你可以生成一个一级URL列表，然后为其指定一个回调处理函数，接着在一级页面提取出二级URL，再为其指定一个回调处理函数然后提取你想要的数据。这个就是一般爬虫开发的思路。在生成URL列表时，就会有深度优先还是广度优先的策略，在scrapy这样的框架中，URL是由一个队列来维护的，所以可以说scrapy是深度优先策略（可以在settings.py中设置成广度优先）。从开发者角度看，大多数网站都是一个一级页面，在页面上有二级页面的简略信息列表，所以你可以在一级页面中提取出二级页面的URL，然后依次发出这些URL的GET/POST请求，等这些URL的页面下载好时进入回调函数提取数据。在提取页面数据时，如果使用scrapy这样的框架，你可以采用xpath这种语法指定某个或某类html节点获取该节点，该节点的属性或文本等。选取节点的工具还有pyjquery，beautiful soup等，当然你可以使用正则表达式以及字符串方法处理，毕竟下载下来的网页源码是文本格式。提取了一个完整的数据结构体之后，可以将其保存在数据库，json/csv/xml，文本等地方。

- **2，查重与增量爬取。** 在提取URL时，可能有些URL是我们已经爬过的或者是重复提取的，所以这时要对URL查重。在scrapy框架中，已经对URL进行了查重，如果你是用requests, urllib这样的工具做爬虫，URL的查重可以使用集合，布隆过滤器等工具。区别是集合在URL特别多时消耗的内存多，速度会变慢，如果程序重新启动那么集合里的数据便没有了。布隆过滤器需要的内存较少，且随着URL的增多也不会增加内存的消耗，但它一般需要额外的文件来持久化爬取过的URL的信息，不利于程序在不同机器上迁移。在查重时，可以在URL层面进行，也可以在数据库层面进行，例如每次插入前查询是否已有该数据，然后选择进行插入，更新或者抛弃数据。在数据库层面进行查询以现在的数据库性能来说不会成为爬虫性能的瓶颈，因此是一个实用的方法。如果你爬取了一个网站的全部数据，但是这个网站是每天更新的，所以你需要每天或者一段时间后进行对这些更新的增量进行爬取。这个任务关键的地方就是查重，识别爬取过的内容，然后爬取，保存新的数据。增量爬取的方法可以选择刚才所说的查重方法，然后在发生重复的地方关闭程序，对于乱序的数据，可以设置一个容错阈值，比如连续重复了n次说明接下来的数据都是已经爬取过的，此时应该关闭程序。

- **3，请求头，cookies，POST数据，URL参数。** 如果你在网上搜索一些爬虫开发经验帖，教程帖，就会经常碰到这些话题，仿佛这些是爬虫的技术难点一样。诚然，对于某些网站，某些页面，它需要提交一些数据然后才能进入页面，对于爬虫程序来说这是一个关卡，通过之后才能运作。但是从开发者角度看，大多数的网站，网页都是不需要通过上述关卡的，如何做好爬虫常见任务才是关键。如果遇到需要上述“关卡”，建议就是网站需要什么就给它什么，照着方法做一遍就是。如果使用scrapy，像请求头，cookies等都是默认配置好的，如果需要POST数据，URL参数则很容易在Request中添加进去。

- **4，javascript动态生成的内容，json数据与API。** 对于网页上由js动态生成的数据，如果不是必须抓取的则不必抓取，如果是则可以使用selenium+phantomjs去模拟浏览器抓取。通常这种做法会使程序性能明显下降，很容易崩溃。也可以另外使用一个爬虫专门去抓取这些数据，这样就不会对原有的爬虫造成影响。对于一些页面是由json发送数据再由js（如Reactjs）渲染成页面显示的网站，使用scrapy框架请求该页面未必会返回回调函数执行，此时应该使用一些简单的工具如requests来访问该URL，即该网站的API接口，直接获得json数据后再处理。

- **5，分页。** 分页是一个最常见的网页结构，也是一个爬虫任务的“标配”。常见的分页有顺序式（等差为1），间隔式（等差为n），下一页式（没有分页电梯按钮）等，在爬虫获取分页地址时（常见的是下一页的地址）往往是直接使用xpath工具选取网页上下一页的地址，也可以选取你想要的分页地址，如果网页没有“下一页”按钮，可以通过“当前页”的下一个兄弟节点获得（比较少见），也可以通过自己观察URL结构构造URL来获得。对于一般网站的分页，往往会在数据库查询获得前几十页的结果分好页放在缓存中，所以此时通过分页下载页面很快，一旦超过缓存页面数量，便会促使网站后台进行数据库查询，此时下载页面的速度便会变慢。如果此时设置了超时限制，爬虫可能会触发超时异常。

- **6，易用性与耦合性。** 一个爬虫应该可以简单的使用，开始一个爬虫任务时要从使用者角度看爬虫的要求，比如简单地启动命令（可以加一些参数），比如简单地自动地完成增量爬取，周期爬取，指定区间（时间区间，位置区间）爬取与更新，宿主机器迁移等等常见的需求。爬虫内部程序要解耦，需要生成URL的要与数据提取与清理，数据保存分开，生成一级URL的与二级URL的最好分开等等最基本的解耦意识，否则随着以后需求变复杂，程序会变得难以维护。使用scrapy可以帮助完成这些工作。


